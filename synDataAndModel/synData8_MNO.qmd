---
title: "Multinomial outcomes "
author: "Jeff Wu"
format: html
editor: visual
number-sections: true
number-depth: 3
bibliography: references.bib
---

```{r}
#| echo: false
#| output: false

# Housekeeping
source("https://raw.githubusercontent.com/jeffzifanwu/rProject/main/procedures/beginScript.R")

# load library
library(dplyr)
library(dataHelper)
library(evd)
library(ggplot2)
source("https://raw.githubusercontent.com/jeffzifanwu/rProject/main/procedures/setGgplotThemes.R")
library(tidyr)
```

------------------------------------------------------------------------

# Introduction

Some text

Some maths:

$$
y = \beta x
$$

Some R codes

```{r}
1 + 1
```

# MNL

Let there be three alternatives $\mathcal{D} = \{0,1,2\}$, available to each individual. We use the subscript $i \in \{0,1,2\}$ to index alternative; and superscript $k \in \{1, \ldots,K \}$ to index predictors and coefficients.

**Deterministic indirect utility**

$$
\begin{align}
V_0 & = \alpha^1_{0} \sum_{k=2}^{K}  \alpha^k x_0^{k}
\\
V_1 & = \alpha^1_{1} + \sum_{k=2}^{K} \alpha^2 x_1^{k}
\\
V_2 & = \alpha^1_{2} + \sum_{k=2}^{K} \alpha^2 x_2^{k}
\end{align}
$$

**Total indirect utility**

$$
\begin{align}
V_0^{'} & = V_0 + \epsilon_0
\\
V_1^{'} & = V_1 + \epsilon_1
\\
V_2^{'} & = V_2 + \epsilon_2
\end{align}
$$

where $\epsilon_i$ is i.i.d. extreme value type I (Gumbel).

**Choice rule**

$$
D = 
\begin{cases}
  0 & \text{if } V_0^{'} = \max_{i} V_i^{'} \\
  1 & \text{if } V_1^{'} = \max_{i} V_i^{'}  \\
  2 & \text{if } V_2^{'} = \max_{i} V_i^{'} 
\end{cases}
$$

## Simulation

```{r}

# - - - - - - - - - - - - - - - - - - - - - - - - - - - -
# Specifications
# - - - - - - - - - - - - - - - - - - - - - - - - - - - -

# N individuals
N <- 1000

# J alternatives
J <- 3 

# parameters
alphaTrue = c(
  0,   # alpha1, alternative 0
  1,   # alpha1, alternative 1
  2,   # alpha1, alternative 2
  0.2, # alpha2, alternative 0
  0.2, # alpha2, alternative 1
  0.2, # alpha2, alternative 2
  0.6, # alpha3, alternative 0
  0.6, # alpha3, alternative 1
  0.6  # alpha3, alternative 2
)

# - - - - - - - - - - - - - - - - - - - - - - - - - - - -
# Simulate data
# - - - - - - - - - - - - - - - - - - - - - - - - - - - -

# ids
df.sim = genDimIds(dimSize =  c(N,J), dimName = c(
  "agent_id",
  "alter_id"
  ))

# alter_id starts from 0
df.sim = df.sim %>%
  mutate(alter_id = as.integer(alter_id) - 1)

# predictor x1 - intercept
df.sim <- df.sim %>%
  mutate(x1 = 1)
  
# predictor x2 - agent-specific wage rates
df.agent = df.sim %>%
  distinct(agent_id) %>%
  mutate(x2 = rnorm(N, 50, 10))

df.sim = df.sim %>%
  left_join(df.agent, by = "agent_id")

# predictor x3 - alternative-specific full-income (10 Â£)
df.sim = df.sim %>%
  mutate(x3 = case_when(
    alter_id == 0 ~ rnorm(n(), 10, 1),
    alter_id == 1 ~ rnorm(n(), 9, 1),
    alter_id == 2 ~ rnorm(n(), 8, 1)
  ))

# parameters
df.sim = df.sim %>%
  # alpha1 
  mutate(alpha1 = case_when(
      alter_id == 0 ~ alphaTrue[1],
      alter_id == 1 ~ alphaTrue[2],
      alter_id == 2 ~ alphaTrue[3]
  )) %>%
  # alpha2
  mutate(alpha2 = case_when(
      alter_id == 0 ~ alphaTrue[4],
      alter_id == 1 ~ alphaTrue[5],
      alter_id == 2 ~ alphaTrue[6]
  )) %>%
  # alpha3
  mutate(alpha3 = case_when(
      alter_id == 0 ~ alphaTrue[7],
      alter_id == 1 ~ alphaTrue[8],
      alter_id == 2 ~ alphaTrue[9]
  ))

# deterministic indirect utility
df.sim = df.sim %>%
  mutate(
    v = alpha1 + alpha2 * x2 + alpha3 * x3
  )

# unobserved utilities (error terms)
df.sim = df.sim %>%
  mutate(
    eps = rgumbel(N*J, loc = 0, scale = 1)
  )

# total indirect utility
df.sim = df.sim %>%
  mutate(
    v_total = v + eps
  )

# choices
df.sim = df.sim %>%
  group_by(agent_id) %>%
  mutate(choice = alter_id[which.max(v_total)]) %>%
  ungroup()

# - - - - - - - - - - - - - - - - - - - - - - - - - - - -
# Likelihood
# - These are used for debug checks later
# - - - - - - - - - - - - - - - - - - - - - - - - - - - -

# Probability of choices
df.ll = df.sim %>%
  group_by(agent_id) %>%
  # probability
  mutate(p = exp(v) / sum(exp(v))) %>%
  # is chosen indiciator
  mutate(is_chosen = if_else(alter_id == choice, 1, 0)) %>%
  # log-likelihood
  mutate(ll = is_chosen * log(p))

# calculate total negative log likelihood
nllTrue = -sum(df.ll$ll)
print(paste0("True total negative log-likelihood: ", nllTrue))
```

### Visualising simulated data

```{r}

# plot predictors
hist(df.sim$x2)
hist(df.sim$x3)

# plot discrete choices
df.plot = df.sim %>%
  distinct(agent_id, choice) %>%
  mutate(choice = as.factor(choice))

ggplot(df.plot, aes(x = choice)) +
  geom_bar() +
  labs(
    title = "Choices",
    x = "Choice",
    y = "Count"
  )
```

# Estimation

The probability $y_i = 1$, i.e., choosing the first alternative, is given by:

$$
\begin{align}
  p_i 
  &   = Prob(V_{1}^{'} \geq V_{0}^{'}, V_{1}^{'} \geq V_{2}^{'}) \\
  &   = Prob(\epsilon_{0} \leq \bar{V}_{1,0} + \epsilon_{1}, \epsilon_{2} \leq \bar{V}_{1,2} + \epsilon_{1}) \\
  &   = \int_{z=-\infty}^{\infty}
    \left[
    \int_{y=-\infty}^{\bar{V}_{1,2} + \epsilon_{1}}
      \int_{x=-\infty}^{ \bar{V}_{1,0} + \epsilon_{1}}
        f_{\epsilon_0}(x) f_{\epsilon_2}(y) \, d\epsilon_0 \, d\epsilon_2
    \right]
    f_{\epsilon_1}(z) \, d\epsilon_1
\end{align}
$$

where,

$$
\begin{align}
  \bar{V}_{1,0} 
    & = V_1 - V_0
    \\
  \bar{V}_{1,2} 
    & = V_1 - V_2
\end{align}
$$

$f_{\epsilon_i} (\cdot)$ is the probability density function for $\epsilon_i$. The probability $y_2 = 1$, $y_3 = 1$ can be expressed analogously.

Using the assumptions, that, $\epsilon_{i}$, $\forall i = 1,2,3$, are i.i.d EV1, we derive the close-form MNL probability function [@Train2009]:

$$
p_{i} = \frac
  {exp(V_{i})}
  {\sum_{j=0}^{2} exp(V_{j})}
$$

**Log-likelihood**

$$
ll_n = \sum_{i} y_{i,n} \ln p_{i,n}
$$

where $n$ indexes agents.

**Total negative log-likelihood**

$$
nll = - \sum_{n=1}^{N} ll_n
$$

We estimate the coefficients:

```{r}
# get predictors dataframe
df.dmat = df.sim %>%
  select(
    agent_id,
    alter_id,
    x1, x2, x3
  ) %>%
  pivot_wider(
    names_from = alter_id,
    values_from = c(x1, x2, x3),
    names_sep = "_"
  )

# get design matrix
mat.x = df.dmat %>%
  select(-agent_id) %>%
  as.matrix()

# get choice matrix
mat.y = df.sim %>%
  distinct(agent_id, choice) %>%
  select(choice) %>%
  mutate(y_0 = if_else(choice == 0, 1, 0),
         y_1 = if_else(choice == 1, 1, 0),
         y_2 = if_else(choice == 2, 1, 0)
  ) %>%
  select(-choice) %>%
  as.matrix()

# Define log-likelihood function
nll <- function(beta, mat.x, mat.y) {
  
  # predictors for each alternative
  mat.x_0 = mat.x[, grepl("_0", colnames(mat.x))]
  mat.x_1 = mat.x[, grepl("_1", colnames(mat.x))]
  mat.x_2 = mat.x[, grepl("_2", colnames(mat.x))]
  
  # parameters for each alternative
  beta_0 = beta[c(1,4,7)]
  beta_1 = beta[c(2,5,8)]
  beta_2 = beta[c(3,6,9)]
  
  # utilities for each alternative
  mat.v_0 = mat.x_0 %*% beta_0
  mat.v_1 = mat.x_1 %*% beta_1
  mat.v_2 = mat.x_2 %*% beta_2
  mat.v   = cbind(mat.v_0, mat.v_1, mat.v_2)
  
  # calculate probabilities
  mat.p_0 = exp(mat.v_0) / rowSums(exp(mat.v))
  mat.p_1 = exp(mat.v_1) / rowSums(exp(mat.v))
  mat.p_2 = exp(mat.v_2) / rowSums(exp(mat.v))
  mat.p   = cbind(mat.p_0, mat.p_1, mat.p_2)
  
  # calculate log-likelihood
  mat.ll = mat.y * log(mat.p)
  
  # total negative log-likelihood
  nll = -sum(mat.y * log(mat.p))
  cat("Total negative log-likelihood:", nll, "\n")
  
  return(nll)
  
}

# Debug checks
nllCurr = nll(alphaTrue, mat.x, mat.y) # current total negative log-likelihood
if (!isTRUE(all.equal(nllCurr, nllTrue))) {
  stop("Mismatched total negative log-likelihoods.")
} else {
  cat("total negative log-likelihood:", nllCurr)
}

# - - - - - - - - - - - - - - - - - - - - - - - - - - - -
# Optimisation
# - - - - - - - - - - - - - - - - - - - - - - - - - - - -

# start values
startValues = rep(0,ncol(mat.x))

# Optimise
opt = optim(
  par   = startValues,
  fn    = nll,
  mat.x = mat.x,
  mat.y = mat.y,
  method = "BFGS",
  hessian = TRUE
  )

# point estimates of coefficients
mat.pe = opt$par

# var-cov matrix
mat.varCov = solve(opt$hessian)

# standard errors
mat.se = sqrt(diag(mat.varCov))

# z-scores
mat.zScore = mat.pe / mat.se

# p-values
mat.pValue = 2 * (1 - pnorm(abs(mat.zScore)))

# Display results
df.results = data.frame(
  point_estimate = mat.pe,   
  standard_error = mat.se,   
  z_score        = mat.zScore,
  p_value        = mat.pValue
)
print(df.results)

```


# Others
**Utility differences**

$$
\begin{align}
  \bar{V}_{1} 
    & = \bar{V}_1 - \bar{V}_0
    \\
  \bar{V}_{2} 
    & = \bar{V}_2 - \bar{V}_0
    \\
  \bar{V}^{'}_{1}
    & = V_1^{'} - V_0^{'} = \bar{V}_{1} - \upsilon_{1}
    \\
  \bar{V}^{'}_{2}
    & = V_2^{'} - V_0^{'} = \bar{V}_{2} - \upsilon_{2}
\end{align}
$$

where $\upsilon_{1} = \epsilon_0 - \epsilon_1$ and $\upsilon_{2} = \epsilon_0 - \epsilon_2$.

**MNL probabilities**

$$
p_{i} = \frac
  {exp(V_{i})}
  {\sum_{j=0}^{2} exp(V_{j})}
$$

------------------------------------------------------------------------

```{r}
#| echo: false
#| output: false

# Clean up
# source("https://raw.githubusercontent.com/jeffzifanwu/rProject/main/procedures/endScript.R")
```
